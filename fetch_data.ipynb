{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2076446-2d6a-4717-aae3-943bf9768bce",
   "metadata": {},
   "source": [
    "## Search in pubmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a1c659f-5e2f-4c3a-a94c-9826dfa0a63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching PubMed with query: health open data kenya malaria hiv maternal mental\n",
      "Found 0 articles\n",
      "Data saved to health_data_kenya_pubmed_refined.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Function to perform a PubMed search\n",
    "def search_pubmed(query, max_results=16000):\n",
    "    print(f\"Searching PubMed with query: {query}\")\n",
    "    url = f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term={query}&retmax={max_results}&retmode=json\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    print(f\"Found {len(data['esearchresult']['idlist'])} articles\")\n",
    "    return data['esearchresult']['idlist']\n",
    "\n",
    "# Function to fetch details of a PubMed article\n",
    "def fetch_article_details(pmid):\n",
    "    url = f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?db=pubmed&id={pmid}&retmode=json\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    return data['result'][pmid]\n",
    "\n",
    "# Function to parse the article details and extract required information\n",
    "def parse_article_details(article):\n",
    "    title = article['title']\n",
    "    authors = ', '.join([author['name'] for author in article['authors']])\n",
    "    doi = article.get('elocationid', 'N/A')\n",
    "    abstract_url = f\"https://pubmed.ncbi.nlm.nih.gov/{article['uid']}/\"\n",
    "    abstract_response = requests.get(abstract_url)\n",
    "    abstract_soup = BeautifulSoup(abstract_response.text, 'html.parser')\n",
    "    abstract = abstract_soup.find('div', {'class': 'abstract-content selected'}).text.strip() if abstract_soup.find('div', {'class': 'abstract-content selected'}) else 'N/A'\n",
    "    return {\n",
    "        'Title': title,\n",
    "        'Author': authors,\n",
    "        'DOI': doi,\n",
    "        'Abstract': abstract\n",
    "    }\n",
    "\n",
    "# Text mining functions\n",
    "def extract_research_question(abstract):\n",
    "    sentences = abstract.split('.')\n",
    "    keywords = ['research question', 'study aim', 'objective', 'this study', 'we investigate']\n",
    "    for sentence in sentences:\n",
    "        if any(keyword in sentence.lower() for keyword in keywords):\n",
    "            return sentence.strip()\n",
    "    return 'N/A'\n",
    "\n",
    "def extract_research_gap(abstract):\n",
    "    sentences = abstract.split('.')\n",
    "    keywords = ['however', 'but', 'nevertheless', 'gap', 'challenge', 'unknown']\n",
    "    for sentence in sentences:\n",
    "        if any(keyword in sentence.lower() for keyword in keywords):\n",
    "            return sentence.strip()\n",
    "    return 'N/A'\n",
    "\n",
    "def extract_main_findings(abstract):\n",
    "    sentences = abstract.split('.')\n",
    "    keywords = ['results', 'findings', 'we found', 'our study shows', 'conclusion']\n",
    "    for sentence in sentences:\n",
    "        if any(keyword in sentence.lower() for keyword in keywords):\n",
    "            return sentence.strip()\n",
    "    return 'N/A'\n",
    "\n",
    "# Function to determine if the data is FAIR\n",
    "def is_fair_data(abstract):\n",
    "    return 'Yes' if 'data available' in abstract.lower() or 'open data' in abstract.lower() else 'No'\n",
    "\n",
    "# Function to determine the area of focus\n",
    "def determine_area_of_focus(abstract):\n",
    "    if 'malaria' in abstract.lower():\n",
    "        return 'Malaria'\n",
    "    elif 'hiv' in abstract.lower():\n",
    "        return 'HIV'\n",
    "    elif 'maternal health' in abstract.lower():\n",
    "        return 'Maternal Health'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Function to check if genomic data is available\n",
    "def is_genomic_data_available(abstract):\n",
    "    return 'Yes' if 'genomic' in abstract.lower() else 'No'\n",
    "\n",
    "# Main function to perform the search and generate the CSV file\n",
    "def main():\n",
    "    query = 'health open data kenya malaria hiv maternal mental'\n",
    "    pmids = search_pubmed(query)\n",
    "    articles = []\n",
    "    \n",
    "    for pmid in pmids:\n",
    "        article_details = fetch_article_details(pmid)\n",
    "        parsed_details = parse_article_details(article_details)\n",
    "        parsed_details['FAIR Data'] = is_fair_data(parsed_details['Abstract'])\n",
    "        parsed_details['Area of Focus'] = determine_area_of_focus(parsed_details['Abstract'])\n",
    "        parsed_details['Research Question'] = extract_research_question(parsed_details['Abstract'])\n",
    "        parsed_details['Research Gap'] = extract_research_gap(parsed_details['Abstract'])\n",
    "        parsed_details['Main Findings'] = extract_main_findings(parsed_details['Abstract'])\n",
    "        parsed_details['Genomic Data Available'] = is_genomic_data_available(parsed_details['Abstract'])\n",
    "        articles.append(parsed_details)\n",
    "    \n",
    "    df = pd.DataFrame(articles)\n",
    "    csv_filename = 'health_data_kenya_pubmed_refined.csv'\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "    print(f\"Data saved to {csv_filename}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea9eabd-8c38-4376-bc0d-652d7ba7dded",
   "metadata": {},
   "source": [
    "## Search in google scholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1919e4c0-8cd1-49a5-9874-ed8d75de4768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching Google Scholar with query: Health AND Kenya AND (open data OR Malaria OR HIV OR Maternal health)\n"
     ]
    },
    {
     "ename": "MaxTriesExceededException",
     "evalue": "Cannot Fetch from Google Scholar.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMaxTriesExceededException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 100\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcsv_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 100\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 81\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m     80\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHealth AND Kenya AND (open data OR Malaria OR HIV OR Maternal health)\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 81\u001b[0m     articles \u001b[38;5;241m=\u001b[39m \u001b[43msearch_scholar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     parsed_articles \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m articles:\n",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m, in \u001b[0;36msearch_scholar\u001b[0;34m(query, max_results)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch_scholar\u001b[39m(query, max_results\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSearching Google Scholar with query: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m     search_query \u001b[38;5;241m=\u001b[39m \u001b[43mscholarly\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_pubs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     articles \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_results):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/scholarly/_scholarly.py:160\u001b[0m, in \u001b[0;36m_Scholarly.search_pubs\u001b[0;34m(self, query, patents, citations, year_low, year_high, sort_by, include_last_year, start_index)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Searches by query and returns a generator of Publication objects\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m:param query: terms to be searched\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    155\u001b[0m \n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_url(_PUBSEARCH\u001b[38;5;241m.\u001b[39mformat(requests\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mquote(query)), patents\u001b[38;5;241m=\u001b[39mpatents,\n\u001b[1;32m    158\u001b[0m                           citations\u001b[38;5;241m=\u001b[39mcitations, year_low\u001b[38;5;241m=\u001b[39myear_low, year_high\u001b[38;5;241m=\u001b[39myear_high,\n\u001b[1;32m    159\u001b[0m                           sort_by\u001b[38;5;241m=\u001b[39msort_by, include_last_year\u001b[38;5;241m=\u001b[39minclude_last_year, start_index\u001b[38;5;241m=\u001b[39mstart_index)\n\u001b[0;32m--> 160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__nav\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_publications\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/scholarly/_navigator.py:296\u001b[0m, in \u001b[0;36mNavigator.search_publications\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch_publications\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _SearchScholarIterator:\n\u001b[1;32m    289\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a Publication Generator given a url\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    :param url: the url where publications can be found.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;124;03m    :rtype: {_SearchScholarIterator}\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_SearchScholarIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/scholarly/publication_parser.py:53\u001b[0m, in \u001b[0;36m_SearchScholarIterator.__init__\u001b[0;34m(self, nav, url)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pubtype \u001b[38;5;241m=\u001b[39m PublicationSource\u001b[38;5;241m.\u001b[39mPUBLICATION_SEARCH_SNIPPET \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/scholar?\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m url \u001b[38;5;28;01melse\u001b[39;00m PublicationSource\u001b[38;5;241m.\u001b[39mJOURNAL_CITATION_LIST\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nav \u001b[38;5;241m=\u001b[39m nav\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_total_results()\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpub_parser \u001b[38;5;241m=\u001b[39m PublicationParser(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nav)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/scholarly/publication_parser.py:59\u001b[0m, in \u001b[0;36m_SearchScholarIterator._load_url\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_url\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# this is temporary until setup json file\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_soup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nav\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_soup\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgs_r gs_or gs_scl\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgsc_mpat_ttl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/scholarly/_navigator.py:239\u001b[0m, in \u001b[0;36mNavigator._get_soup\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_soup\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BeautifulSoup:\n\u001b[1;32m    238\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the BeautifulSoup for a page on scholar.google.com\"\"\"\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m     html \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_page\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttps://scholar.google.com\u001b[39;49m\u001b[38;5;132;43;01m{0}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m     html \u001b[38;5;241m=\u001b[39m html\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xa0\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    241\u001b[0m     res \u001b[38;5;241m=\u001b[39m BeautifulSoup(html, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/scholarly/_navigator.py:190\u001b[0m, in \u001b[0;36mNavigator._get_page\u001b[0;34m(self, pagerequest, premium)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_page(pagerequest, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxTriesExceededException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot Fetch from Google Scholar.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mMaxTriesExceededException\u001b[0m: Cannot Fetch from Google Scholar."
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from scholarly import scholarly\n",
    "\n",
    "# Function to search Google Scholar\n",
    "def search_scholar(query, max_results=10000):\n",
    "    print(f\"Searching Google Scholar with query: {query}\")\n",
    "    search_query = scholarly.search_pubs(query)\n",
    "    articles = []\n",
    "    for i in range(max_results):\n",
    "        try:\n",
    "            article = next(search_query)\n",
    "            articles.append(article)\n",
    "            time.sleep(5)  # Adding a 5-second delay between requests\n",
    "        except StopIteration:\n",
    "            break\n",
    "    print(f\"Found {len(articles)} articles\")\n",
    "    return articles\n",
    "\n",
    "# Function to parse the article details and extract required information\n",
    "def parse_article_details(article):\n",
    "    print(f\"Parsing details for article: {article.bib['title']}\")\n",
    "    title = article.bib['title']\n",
    "    authors = ', '.join(article.bib['author'])\n",
    "    doi = article.bib.get('doi', 'N/A')\n",
    "    abstract = article.bib.get('abstract', 'N/A')\n",
    "    return {\n",
    "        'Title': title,\n",
    "        'Author': authors,\n",
    "        'DOI': doi,\n",
    "        'Abstract': abstract\n",
    "    }\n",
    "\n",
    "# Text mining functions\n",
    "def extract_research_question(abstract):\n",
    "    sentences = abstract.split('.')\n",
    "    keywords = ['research question', 'study aim', 'objective', 'this study', 'we investigate']\n",
    "    for sentence in sentences:\n",
    "        if any(keyword in sentence.lower() for keyword in keywords):\n",
    "            return sentence.strip()\n",
    "    return 'N/A'\n",
    "\n",
    "def extract_research_gap(abstract):\n",
    "    sentences = abstract.split('.')\n",
    "    keywords = ['however', 'but', 'nevertheless', 'gap', 'challenge', 'unknown']\n",
    "    for sentence in sentences:\n",
    "        if any(keyword in sentence.lower() for keyword in keywords):\n",
    "            return sentence.strip()\n",
    "    return 'N/A'\n",
    "\n",
    "def extract_main_findings(abstract):\n",
    "    sentences = abstract.split('.')\n",
    "    keywords = ['results', 'findings', 'we found', 'our study shows', 'conclusion']\n",
    "    for sentence in sentences:\n",
    "        if any(keyword in sentence.lower() for keyword in keywords):\n",
    "            return sentence.strip()\n",
    "    return 'N/A'\n",
    "\n",
    "# Function to determine if the data is FAIR\n",
    "def is_fair_data(abstract):\n",
    "    return 'Yes' if 'data available' in abstract.lower() or 'open data' in abstract.lower() else 'No'\n",
    "\n",
    "# Function to determine the area of focus\n",
    "def determine_area_of_focus(abstract):\n",
    "    if 'malaria' in abstract.lower():\n",
    "        return 'Malaria'\n",
    "    elif 'hiv' in abstract.lower():\n",
    "        return 'HIV'\n",
    "    elif 'maternal health' in abstract.lower():\n",
    "        return 'Maternal Health'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Function to check if genomic data is available\n",
    "def is_genomic_data_available(abstract):\n",
    "    return 'Yes' if 'genomic' in abstract.lower() else 'No'\n",
    "\n",
    "# Main function to perform the search and generate the CSV file\n",
    "def main():\n",
    "    query = 'Health AND Kenya AND (open data OR Malaria OR HIV OR Maternal health)'\n",
    "    articles = search_scholar(query)\n",
    "    parsed_articles = []\n",
    "    \n",
    "    for article in articles:\n",
    "        article_details = parse_article_details(article)\n",
    "        article_details['FAIR Data'] = is_fair_data(article_details['Abstract'])\n",
    "        article_details['Area of Focus'] = determine_area_of_focus(article_details['Abstract'])\n",
    "        article_details['Research Question'] = extract_research_question(article_details['Abstract'])\n",
    "        article_details['Research Gap'] = extract_research_gap(article_details['Abstract'])\n",
    "        article_details['Main Findings'] = extract_main_findings(article_details['Abstract'])\n",
    "        article_details['Genomic Data Available'] = is_genomic_data_available(article_details['Abstract'])\n",
    "        parsed_articles.append(article_details)\n",
    "    \n",
    "    df = pd.DataFrame(parsed_articles)\n",
    "    csv_filename = 'health_data_kenya_google_scholar1.csv'\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "    print(f\"Data saved to {csv_filename}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c7a3a2-7005-4d9f-ac2a-c246677f98f5",
   "metadata": {},
   "source": [
    "## Refined pubmed search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9be48812-052b-4f45-84af-8f6a5af71aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching PubMed with query: (\"health\" AND \"open data\" AND \"Kenya\") OR \"malaria\" OR \"HIV\" OR \"maternal health\" OR \"mental health\"\n",
      "Found 100 articles\n",
      "Data saved to health_data_kenya_pubmed_refined.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import quote\n",
    "\n",
    "#requests: for making HTTP requests.\n",
    "#pandas: for data manipulation and analysis.\n",
    "#BeautifulSoup: from the bs4 library, for parsing HTML and extracting data.\n",
    "#quote: from urllib.parse, for URL encoding.\n",
    "\n",
    "# Function to perform a PubMed search\n",
    "def search_pubmed(query, max_results=50):\n",
    "    print(f\"Searching PubMed with query: {query}\")\n",
    "    encoded_query = quote(query) #Encodes the query for use in a URL\n",
    "    url = f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term={encoded_query}&retmax={max_results}&retmode=json\"\n",
    "    response = requests.get(url) #Sends a GET request to the constructed URL\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(\"Error fetching search results:\", response.status_code)\n",
    "        return []\n",
    "    \n",
    "    data = response.json() #Parses the response JSON data into a Python dictionary.\n",
    "    if 'esearchresult' not in data or 'idlist' not in data['esearchresult']: #Checks if the expected keys are present in the parsed data.\n",
    "        print(\"No articles found.\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"Found {len(data['esearchresult']['idlist'])} articles\")\n",
    "    return data['esearchresult']['idlist'] #Returns the list of article IDs found in the search.\n",
    "\n",
    "# Function to fetch details of a PubMed article\n",
    "def fetch_article_details(pmid): #Fetches details of the given pubmed ids\n",
    "    url = f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?db=pubmed&id={pmid}&retmode=json\" #Fetches detailed information about a specified article\n",
    "    response = requests.get(url) #Sends a get request\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error fetching article details for PMCID {pmid}:\", response.status_code)\n",
    "        return None\n",
    "    \n",
    "    data = response.json()\n",
    "    return data['result'].get(pmid, None)\n",
    "\n",
    "# Function to parse the article details and extract required information\n",
    "def parse_article_details(article): #takes an article dictionary.\n",
    "    title = article['title']\n",
    "    authors = ', '.join([author['name'] for author in article['authors']]) if 'authors' in article else 'N/A'\n",
    "    doi = article.get('elocationid', 'N/A')\n",
    "    abstract_url = f\"https://pubmed.ncbi.nlm.nih.gov/{article['uid']}/\" #url for the article abstract\n",
    "    \n",
    "    abstract_response = requests.get(abstract_url) # sends a get request \n",
    "    abstract_soup = BeautifulSoup(abstract_response.text, 'html.parser') #Parses the HTML content of the abstract page\n",
    "    \n",
    "    abstract = abstract_soup.find('div', {'class': 'abstract-content selected'}) #Finds the div element containing the abstract text.\n",
    "    abstract_text = abstract.text.strip() if abstract else 'N/A' #Extracts the abstract text and strips any extra whitespace; defaults to 'N/A' if not found.\n",
    "    \n",
    "    open_source = 'Yes' if abstract_soup.find('a', {'class': 'linkout-link'}) else 'No' #Checks for the presence of a link indicating open access to the article\n",
    "    \n",
    "    return {\n",
    "        'Title': title,\n",
    "        'Author': authors,\n",
    "        'DOI': doi,\n",
    "        'Abstract': abstract_text,\n",
    "        'Open Source': open_source\n",
    "    } #Returns a dictionary containing the parsed article details.\n",
    "\n",
    "# Text mining functions to handle structured abstracts\n",
    "def extract_section(abstract, section_keywords):\n",
    "    sentences = abstract.split('.')\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if any(keyword.lower() in sentence.lower() for keyword in section_keywords):\n",
    "            return '. '.join(sentences[i:i+2]).strip()\n",
    "    return 'N/A'\n",
    "\n",
    "def extract_research_question(abstract):\n",
    "    keywords = ['research question', 'study aim', 'objective', 'this study', 'we investigate']\n",
    "    return extract_section(abstract, keywords)\n",
    "\n",
    "def extract_research_gap(abstract):\n",
    "    keywords = ['however', 'but', 'nevertheless', 'gap', 'challenge', 'unknown']\n",
    "    return extract_section(abstract, keywords)\n",
    "\n",
    "def extract_main_findings(abstract):\n",
    "    keywords = ['results', 'findings', 'we found', 'our study shows', 'conclusion']\n",
    "    return extract_section(abstract, keywords)\n",
    "\n",
    "# Function to determine if the data is FAIR\n",
    "def is_fair_data(abstract):\n",
    "    fair_keywords = ['data available', 'open data', 'FAIR', 'accessible data', 'interoperable data', 'reusable data']\n",
    "    return 'Yes' if any(keyword.lower() in abstract.lower() for keyword in fair_keywords) else 'No'\n",
    "\n",
    "# Function to determine the area of focus\n",
    "def determine_area_of_focus(abstract):\n",
    "    if 'malaria' in abstract.lower():\n",
    "        return 'Malaria'\n",
    "    elif 'hiv' in abstract.lower():\n",
    "        return 'HIV'\n",
    "    elif 'maternal health' in abstract.lower() or 'maternal' in abstract.lower():\n",
    "        return 'Maternal Health'\n",
    "    elif 'mental health' in abstract.lower() or 'mental' in abstract.lower():\n",
    "        return 'Mental Health'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Function to check if genomic data is available\n",
    "def is_genomic_data_available(abstract):\n",
    "    genomic_keywords = ['genomic', 'sequence', 'amplicons', 'Illumina', 'DNA', 'RNA', 'sequencing']\n",
    "    return 'Yes' if any(keyword.lower() in abstract.lower() for keyword in genomic_keywords) else 'No'\n",
    "\n",
    "# Main function to perform the search and generate the CSV file\n",
    "def main():\n",
    "    query = '(\"health\" AND \"open data\" AND \"Kenya\") OR \"malaria\" OR \"HIV\" OR \"maternal health\" OR \"mental health\"'\n",
    "    pmids = search_pubmed(query, max_results=100)  # Limit results for testing\n",
    "    articles = []\n",
    "    \n",
    "    for pmid in pmids:\n",
    "        article_details = fetch_article_details(pmid)\n",
    "        if article_details:\n",
    "            parsed_details = parse_article_details(article_details)\n",
    "            parsed_details['FAIR Data'] = is_fair_data(parsed_details['Abstract'])\n",
    "            parsed_details['Area of Focus'] = determine_area_of_focus(parsed_details['Abstract'])\n",
    "            parsed_details['Research Question'] = extract_research_question(parsed_details['Abstract'])\n",
    "            parsed_details['Research Gap'] = extract_research_gap(parsed_details['Abstract'])\n",
    "            parsed_details['Main Findings'] = extract_main_findings(parsed_details['Abstract'])\n",
    "            parsed_details['Genomic Data Available'] = is_genomic_data_available(parsed_details['Abstract'])\n",
    "            articles.append(parsed_details)\n",
    "    \n",
    "    df = pd.DataFrame(articles)\n",
    "    csv_filename = 'health_data_kenya_pubmed_refined.csv'\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "    print(f\"Data saved to {csv_filename}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c17726-688a-47e9-9ba4-2d3042ab04a0",
   "metadata": {},
   "source": [
    "## Refine google scholar searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71d1ae1b-5eb4-4922-ade8-1d8862e25757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching Google Scholar with query: Health AND Kenya AND (open data OR Malaria OR HIV OR Maternal health)\n"
     ]
    },
    {
     "ename": "MaxTriesExceededException",
     "evalue": "Cannot Fetch from Google Scholar.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMaxTriesExceededException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 109\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcsv_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 109\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 90\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m     89\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHealth AND Kenya AND (open data OR Malaria OR HIV OR Maternal health)\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 90\u001b[0m     articles \u001b[38;5;241m=\u001b[39m \u001b[43msearch_scholar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     parsed_articles \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m articles:\n",
      "Cell \u001b[0;32mIn[17], line 9\u001b[0m, in \u001b[0;36msearch_scholar\u001b[0;34m(query, max_results)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch_scholar\u001b[39m(query, max_results\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSearching Google Scholar with query: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m     search_query \u001b[38;5;241m=\u001b[39m \u001b[43mscholarly\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_pubs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     articles \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_results):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/scholarly/_scholarly.py:160\u001b[0m, in \u001b[0;36m_Scholarly.search_pubs\u001b[0;34m(self, query, patents, citations, year_low, year_high, sort_by, include_last_year, start_index)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Searches by query and returns a generator of Publication objects\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m:param query: terms to be searched\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    155\u001b[0m \n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_url(_PUBSEARCH\u001b[38;5;241m.\u001b[39mformat(requests\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mquote(query)), patents\u001b[38;5;241m=\u001b[39mpatents,\n\u001b[1;32m    158\u001b[0m                           citations\u001b[38;5;241m=\u001b[39mcitations, year_low\u001b[38;5;241m=\u001b[39myear_low, year_high\u001b[38;5;241m=\u001b[39myear_high,\n\u001b[1;32m    159\u001b[0m                           sort_by\u001b[38;5;241m=\u001b[39msort_by, include_last_year\u001b[38;5;241m=\u001b[39minclude_last_year, start_index\u001b[38;5;241m=\u001b[39mstart_index)\n\u001b[0;32m--> 160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__nav\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_publications\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/scholarly/_navigator.py:296\u001b[0m, in \u001b[0;36mNavigator.search_publications\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch_publications\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _SearchScholarIterator:\n\u001b[1;32m    289\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a Publication Generator given a url\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    :param url: the url where publications can be found.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;124;03m    :rtype: {_SearchScholarIterator}\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_SearchScholarIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/scholarly/publication_parser.py:53\u001b[0m, in \u001b[0;36m_SearchScholarIterator.__init__\u001b[0;34m(self, nav, url)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pubtype \u001b[38;5;241m=\u001b[39m PublicationSource\u001b[38;5;241m.\u001b[39mPUBLICATION_SEARCH_SNIPPET \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/scholar?\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m url \u001b[38;5;28;01melse\u001b[39;00m PublicationSource\u001b[38;5;241m.\u001b[39mJOURNAL_CITATION_LIST\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nav \u001b[38;5;241m=\u001b[39m nav\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_total_results()\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpub_parser \u001b[38;5;241m=\u001b[39m PublicationParser(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nav)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/scholarly/publication_parser.py:59\u001b[0m, in \u001b[0;36m_SearchScholarIterator._load_url\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_url\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# this is temporary until setup json file\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_soup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nav\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_soup\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgs_r gs_or gs_scl\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgsc_mpat_ttl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/scholarly/_navigator.py:239\u001b[0m, in \u001b[0;36mNavigator._get_soup\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_soup\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BeautifulSoup:\n\u001b[1;32m    238\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the BeautifulSoup for a page on scholar.google.com\"\"\"\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m     html \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_page\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttps://scholar.google.com\u001b[39;49m\u001b[38;5;132;43;01m{0}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m     html \u001b[38;5;241m=\u001b[39m html\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xa0\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    241\u001b[0m     res \u001b[38;5;241m=\u001b[39m BeautifulSoup(html, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/scholarly/_navigator.py:190\u001b[0m, in \u001b[0;36mNavigator._get_page\u001b[0;34m(self, pagerequest, premium)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_page(pagerequest, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxTriesExceededException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot Fetch from Google Scholar.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mMaxTriesExceededException\u001b[0m: Cannot Fetch from Google Scholar."
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from scholarly import scholarly\n",
    "\n",
    "# Function to search Google Scholar\n",
    "def search_scholar(query, max_results=50):\n",
    "    print(f\"Searching Google Scholar with query: {query}\")\n",
    "    search_query = scholarly.search_pubs(query)\n",
    "    articles = []\n",
    "    for i in range(max_results):\n",
    "        try:\n",
    "            article = next(search_query)\n",
    "            articles.append(article)\n",
    "            time.sleep(random.uniform(5, 10))  # Adding a random delay between 5 and 10 seconds\n",
    "        except StopIteration:\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Exception encountered: {e}\")\n",
    "            print(\"Waiting before retrying...\")\n",
    "            time.sleep(60)  # Wait for a minute before retrying\n",
    "            search_query = scholarly.search_pubs(query)  # Re-initialize the search\n",
    "    print(f\"Found {len(articles)} articles\")\n",
    "    return articles\n",
    "\n",
    "# Function to parse the article details and extract required information\n",
    "def parse_article_details(article):\n",
    "    print(f\"Parsing details for article: {article.bib['title']}\")\n",
    "    title = article.bib['title']\n",
    "    authors = ', '.join(article.bib['author'])\n",
    "    doi = article.bib.get('doi', 'N/A')\n",
    "    abstract = article.bib.get('abstract', 'N/A')\n",
    "    \n",
    "    open_source = 'Yes' if article.bib.get('url') and ('.pdf' in article.bib.get('url') or '.html' in article.bib.get('url')) else 'No'\n",
    "    \n",
    "    return {\n",
    "        'Title': title,\n",
    "        'Author': authors,\n",
    "        'DOI': doi,\n",
    "        'Abstract': abstract,\n",
    "        'Open Source': open_source\n",
    "    }\n",
    "\n",
    "# Text mining functions to handle structured abstracts\n",
    "def extract_section(abstract, section_keywords):\n",
    "    sentences = abstract.split('.')\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if any(keyword.lower() in sentence.lower() for keyword in section_keywords):\n",
    "            return '. '.join(sentences[i:i+2]).strip()\n",
    "    return 'N/A'\n",
    "\n",
    "def extract_research_question(abstract):\n",
    "    keywords = ['research question', 'study aim', 'objective', 'this study', 'we investigate']\n",
    "    return extract_section(abstract, keywords)\n",
    "\n",
    "def extract_research_gap(abstract):\n",
    "    keywords = ['however', 'but', 'nevertheless', 'gap', 'challenge', 'unknown']\n",
    "    return extract_section(abstract, keywords)\n",
    "\n",
    "def extract_main_findings(abstract):\n",
    "    keywords = ['results', 'findings', 'we found', 'our study shows', 'conclusion']\n",
    "    return extract_section(abstract, keywords)\n",
    "\n",
    "# Function to determine if the data is FAIR\n",
    "def is_fair_data(abstract):\n",
    "    fair_keywords = ['data available', 'open data', 'FAIR', 'accessible data', 'interoperable data', 'reusable data']\n",
    "    return 'Yes' if any(keyword.lower() in abstract.lower() for keyword in fair_keywords) else 'No'\n",
    "\n",
    "# Function to determine the area of focus\n",
    "def determine_area_of_focus(abstract):\n",
    "    if 'malaria' in abstract.lower():\n",
    "        return 'Malaria'\n",
    "    elif 'hiv' in abstract.lower():\n",
    "        return 'HIV'\n",
    "    elif 'maternal health' in abstract.lower() or 'maternal' in abstract.lower():\n",
    "        return 'Maternal Health'\n",
    "    elif 'mental health' in abstract.lower() or 'mental' in abstract.lower():\n",
    "        return 'Mental Health'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Function to check if genomic data is available\n",
    "def is_genomic_data_available(abstract):\n",
    "    genomic_keywords = ['genomic', 'sequence', 'amplicons', 'Illumina', 'DNA', 'RNA', 'sequencing']\n",
    "    return 'Yes' if any(keyword.lower() in abstract.lower() for keyword in genomic_keywords) else 'No'\n",
    "\n",
    "# Main function to perform the search and generate the CSV file\n",
    "def main():\n",
    "    query = 'Health AND Kenya AND (open data OR Malaria OR HIV OR Maternal health)'\n",
    "    articles = search_scholar(query)\n",
    "    parsed_articles = []\n",
    "    \n",
    "    for article in articles:\n",
    "        article_details = parse_article_details(article)\n",
    "        article_details['FAIR Data'] = is_fair_data(article_details['Abstract'])\n",
    "        article_details['Area of Focus'] = determine_area_of_focus(article_details['Abstract'])\n",
    "        article_details['Research Question'] = extract_research_question(article_details['Abstract'])\n",
    "        article_details['Research Gap'] = extract_research_gap(article_details['Abstract'])\n",
    "        article_details['Main Findings'] = extract_main_findings(article_details['Abstract'])\n",
    "        article_details['Genomic Data Available'] = is_genomic_data_available(article_details['Abstract'])\n",
    "        parsed_articles.append(article_details)\n",
    "    \n",
    "    df = pd.DataFrame(parsed_articles)\n",
    "    csv_filename = 'health_data_kenya_google_scholar_refined.csv'\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "    print(f\"Data saved to {csv_filename}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cde5058-5e18-4492-83e4-285f8c3ca156",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
